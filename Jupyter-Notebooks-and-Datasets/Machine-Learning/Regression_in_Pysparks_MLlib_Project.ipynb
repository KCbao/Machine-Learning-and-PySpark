{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in PySpark's MLlib Project\n",
    "\n",
    "Now it's time to put what you've learned to into action with a REAL project! \n",
    "\n",
    "You have been hired as a consultant to a cement production company who wants to be able to improve their customer experience around a number of areas like being able to provide recommendations to cusomters on optimal amounts of certian ingredients in the cement making process and perhaps even create an application where users can input their own values and received a predicted cement strength!\n",
    "\n",
    "I have provided a list of question below to help guide you through this project but feel free to deviate and make this project your own! But first, a bit about this dataset.\n",
    "\n",
    "### About this dataset \n",
    "This dataset contains 1030 instances of concrete samples, containing 9 attributes (8 continuous and 1 discreate), and 1 continuous quantitative output variable. There are no missing attribute values.\n",
    "\n",
    "I've also provided the variable name, variable type, the measurement unit and a brief description of each variable in the dataset. The concrete compressive strength is the outcome variable for our analysis. The order of this listing corresponds to the order of numerals along the rows of the database.\n",
    "\n",
    "Name -- Data Type -- Measurement -- Description\n",
    "\n",
    "- Cement -- quantitative -- kg in a m3 mixture -- Input Variable \n",
    "- Blast Furnace Slag -- quantitative -- kg in a m3 mixture -- Input Variable \n",
    "- Fly Ash -- quantitative -- kg in a m3 mixture -- Input Variable \n",
    "- Water -- quantitative -- kg in a m3 mixture -- Input Variable \n",
    "- Superplasticizer -- quantitative -- kg in a m3 mixture -- Input Variable \n",
    "- Coarse Aggregate -- quantitative -- kg in a m3 mixture -- Input Variable \n",
    "- Fine Aggregate -- quantitative -- kg in a m3 mixture -- Input Variable \n",
    "- Age -- quantitative -- Day (1~365) -- Input Variable \n",
    "- Concrete compressive strength -- quantitative -- MPa -- Output Variable\n",
    "\n",
    "**Source:** https://www.kaggle.com/maajdl/yeh-concret-data\n",
    "\n",
    "**Dataset Name:** Concrete_Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://casiebaodembp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Regression</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f975d147d50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Regression\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dependencies\n",
    "\n",
    "# For data prep\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# To check for multicolinearity\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# For training and evaluation\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "This is a dataset containing housing pricing data for California. Each row of data represents the median statistics for a block (eg. median income, median age of house, etc.). You could this data in a number of ways, but we will use it to predict the median house value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"Datasets/\"\n",
    "df = spark.read.csv(path+'Concrete_Data.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>90</td>\n",
       "      <td>47.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0   540.0    0.0     0.0  162.0               2.5           1040.0   \n",
       "1   540.0    0.0     0.0  162.0               2.5           1055.0   \n",
       "2   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "3   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "4   198.6  132.4     0.0  192.0               0.0            978.4   \n",
       "5   266.0  114.0     0.0  228.0               0.0            932.0   \n",
       "\n",
       "   fineaggregate  age  csMPa  \n",
       "0          676.0   28  79.99  \n",
       "1          676.0   28  61.89  \n",
       "2          594.0  270  40.27  \n",
       "3          594.0  365  41.05  \n",
       "4          825.5  360  44.30  \n",
       "5          670.0   90  47.03  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cement: double (nullable = true)\n",
      " |-- slag: double (nullable = true)\n",
      " |-- flyash: double (nullable = true)\n",
      " |-- water: double (nullable = true)\n",
      " |-- superplasticizer: double (nullable = true)\n",
      " |-- coarseaggregate: double (nullable = true)\n",
      " |-- fineaggregate: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- csMPa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Starting\n",
    "print(df.count())\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop missing data\n",
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Data\n",
    "MLlib requires all input columns of your dataframe to be vectorized. You will see that we rename our dependent var to label as that is what is expected for all MLlib applications. If we rename once here, we won't need to specify it later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseaggregate', 'fineaggregate', 'age']\n",
    "dependent_var = 'csMPa' # Concrete compressive strength -- quantitative -- MPa -- Output Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = df.withColumnRenamed(dependent_var,'label')\n",
    "\n",
    "# Make sure dependent variable is numeric and change if it's not\n",
    "if str(renamed.schema['label'].dataType) != 'IntegerType':\n",
    "    renamed = renamed.withColumn(\"label\", renamed[\"label\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data in the input column list to numeric\n",
    "# Otherwise the Algorithm will not be able to process it, but from df.schema, all input columns are non-string type\n",
    "\n",
    "# First create empty lists set up to divide you input list into numeric and string data types\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    if str(renamed.schema[column].dataType) == 'StringType':\n",
    "        new_col_name = column+\"_num\"\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        numeric_inputs.append(column)\n",
    "        indexed = renamed\n",
    "        \n",
    "# If the dataframe contains string types\n",
    "if len(string_inputs) != 0: \n",
    "    # Then use the string indexer to convert them to numeric\n",
    "    # Be careful not to convert a continuous variable that was read in incorrectly here\n",
    "    # This is meant for categorical columns only\n",
    "    for column in input_columns:\n",
    "        if str(renamed.schema[column].dataType) == 'StringType':\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "            indexed = indexer.fit(renamed).transform(renamed)\n",
    "else:\n",
    "    indexed = renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cement: double (nullable = true)\n",
      " |-- slag: double (nullable = true)\n",
      " |-- flyash: double (nullable = true)\n",
      " |-- water: double (nullable = true)\n",
      " |-- superplasticizer: double (nullable = true)\n",
      " |-- coarseaggregate: double (nullable = true)\n",
      " |-- fineaggregate: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- csMPa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat outliers\n",
    "Test outliers by skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age has been treated for positive (right) skewness. (skew =) 3.2644145354168086 )\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary d\n",
    "d = {}\n",
    "# Create a dictionary of percentiles you want to set\n",
    "# We do top and bottom 1 % which is pretty common\n",
    "for col in numeric_inputs: \n",
    "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "#Now fill in the values\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect() # returns [Row(skewness(age)=-0.0050771183309978015)]\n",
    "    skew = skew[0][0]\n",
    "    # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "    if skew > 1:\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        log(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] ) +1).alias(col))\n",
    "        print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "    elif skew < -1:\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] )).alias(col))\n",
    "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[540.0,0.0,0.0,16...|79.99|\n",
      "|[540.0,0.0,0.0,16...|61.89|\n",
      "|[332.5,142.5,0.0,...|40.27|\n",
      "|[332.5,142.5,0.0,...|41.05|\n",
      "|[198.6,132.4,0.0,...| 44.3|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list = numeric_inputs + string_inputs\n",
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "final_data = assembler.transform(indexed).select('features','label')\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.stat import Correlation\n",
    "pearsonCorr = Correlation.corr(final_data, 'features', 'pearson').collect()[0][0]\n",
    "array = pearsonCorr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.27521591 -0.39746734 -0.08158675  0.09238617 -0.10934899\n",
      " -0.22271785  0.00333903]\n",
      "[-0.27521591  1.         -0.3235799   0.10725203  0.04327042 -0.28399861\n",
      " -0.28160267 -0.02088054]\n",
      "[-0.39746734 -0.3235799   1.         -0.25698402  0.37750315 -0.00996083\n",
      "  0.07910849 -0.01974459]\n",
      "[-0.08158675  0.10725203 -0.25698402  1.         -0.65753291 -0.1822936\n",
      " -0.45066117  0.17021254]\n",
      "[ 0.09238617  0.04327042  0.37750315 -0.65753291  1.         -0.26599915\n",
      "  0.22269123 -0.04845305]\n",
      "[-0.10934899 -0.28399861 -0.00996083 -0.1822936  -0.26599915  1.\n",
      " -0.17848096 -0.03813431]\n",
      "[-0.22271785 -0.28160267  0.07910849 -0.45066117  0.22269123 -0.17848096\n",
      "  1.         -0.1148533 ]\n",
      "[ 0.00333903 -0.02088054 -0.01974459  0.17021254 -0.04845305 -0.03813431\n",
      " -0.1148533   1.        ]\n"
     ]
    }
   ],
   "source": [
    "for item in array:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most collinearities are not close to 1, meaning not much collinearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model\n",
    "regressor = LinearRegression()\n",
    "fitModel = regressor.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which evaluator you want to use\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 6.83601\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = fitModel.transform(test)\n",
    "# Select (prediction, true label) and compute test error\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Model Summary WITH cross validation:\u001b[0m\n",
      " \n",
      "Coefficient Standard Errors: \n",
      "+----------------+--------------------+\n",
      "|feature         |coeff std error     |\n",
      "+----------------+--------------------+\n",
      "|age             |0.25368504212065546 |\n",
      "|superplasticizer|0.07763752053902384 |\n",
      "|water           |0.03327856998978411 |\n",
      "|flyash          |0.010421801740941804|\n",
      "|fineaggregate   |0.008928536105276557|\n",
      "|slag            |0.00853588865658646 |\n",
      "|coarseaggregate |0.007905942359760889|\n",
      "|cement          |0.006990544512969447|\n",
      "+----------------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "P Values: \n",
      "+----------------+---------------------+\n",
      "|feature         |P-Value              |\n",
      "+----------------+---------------------+\n",
      "|superplasticizer|0.06474994996048977  |\n",
      "|water           |3.808872239465799E-4 |\n",
      "|coarseaggregate |1.026601010696293E-4 |\n",
      "|fineaggregate   |2.5251104247558942E-5|\n",
      "|slag            |0.0                  |\n",
      "|flyash          |0.0                  |\n",
      "|cement          |0.0                  |\n",
      "|age             |0.0                  |\n",
      "+----------------+---------------------+\n",
      "\n",
      "None\n",
      " \n",
      "RMSE: 6.832143025214056\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "#Now train with cross val\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "             .addGrid(regressor.maxIter, [10, 15]) \\\n",
    "             .addGrid(regressor.regParam, [0.1, 0.01]) \\\n",
    "             .build())\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "\n",
    "print('\\033[1m' + \"Linear Regression Model Summary WITH cross validation:\"+ '\\033[0m')\n",
    "print(\" \")\n",
    "# Run cross validations\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Extract Best model\n",
    "LR_BestModel = fitModel.bestModel\n",
    "\n",
    "# Get Model Summary Statistics\n",
    "# ModelSummary = fitModel.bestModel.summary\n",
    "ModelSummary = LR_BestModel.summary\n",
    "print(\"Coefficient Standard Errors: \")\n",
    "coeff_ste = ModelSummary.coefficientStandardErrors\n",
    "result = spark.createDataFrame(zip(input_columns,coeff_ste), schema=['feature','coeff std error'])\n",
    "print(result.orderBy(result[\"coeff std error\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "print(\"P Values: \") \n",
    "# Then zip with input_columns list and create a df\n",
    "pvalues = ModelSummary.pValues\n",
    "result = spark.createDataFrame(zip(input_columns,pvalues), schema=['feature','P-Value'])\n",
    "print(result.orderBy(result[\"P-Value\"].desc()).show(truncate=False))\n",
    "print(\" \")\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "ModelPredictions = fitModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "test_results = evaluator.evaluate(ModelPredictions)\n",
    "print('RMSE:', test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees with Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+----------------+--------------------+\n",
      "|feature         |score               |\n",
      "+----------------+--------------------+\n",
      "|cement          |0.4144434403678969  |\n",
      "|age             |0.33727459247338565 |\n",
      "|water           |0.12224632323004572 |\n",
      "|slag            |0.09080754673369798 |\n",
      "|superplasticizer|0.022700479648649442|\n",
      "|fineaggregate   |0.007251812489144854|\n",
      "|flyash          |0.005275805057179408|\n",
      "|coarseaggregate |0.0                 |\n",
      "+----------------+--------------------+\n",
      "\n",
      "None\n",
      "8.079072545284056\n"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "# Build your parameter grid\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                         .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20, 40]) \\\n",
    "                         .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
    "                          numFolds=2) # 3 is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "DT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees \n",
    "# in the ensemble The importance vector is normalized to sum to 1. \n",
    "print(\" \")\n",
    "print('\\033[1m' + \" Feature Importances\"+ '\\033[0m')\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "DT_featureImportances = DT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in DT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Trees with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+----------------+-------------------+\n",
      "|feature         |score              |\n",
      "+----------------+-------------------+\n",
      "|age             |0.3392210996154651 |\n",
      "|cement          |0.2181859321043798 |\n",
      "|water           |0.11790024644366599|\n",
      "|superplasticizer|0.1058330294105804 |\n",
      "|slag            |0.07833915379734172|\n",
      "|fineaggregate   |0.06501773831794436|\n",
      "|coarseaggregate |0.04182782129306597|\n",
      "|flyash          |0.03367497901755669|\n",
      "+----------------+-------------------+\n",
      "\n",
      "None\n",
      "4.754161054525083\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                           .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "                           .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "               .addGrid(regressor.numTrees, [5, 20])\n",
    "             .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "RF_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees \n",
    "# in the ensemble The importance vector is normalized to sum to 1. \n",
    "print(\" \")\n",
    "print('\\033[1m' + \" Feature Importances\"+ '\\033[0m')\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "RF_featureImportances = RF_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in RF_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-Boosted Trees with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+----------------+--------------------+\n",
      "|feature         |score               |\n",
      "+----------------+--------------------+\n",
      "|age             |0.262126953881261   |\n",
      "|cement          |0.2292648142791066  |\n",
      "|slag            |0.12048541929525121 |\n",
      "|water           |0.11927045344703106 |\n",
      "|superplasticizer|0.09261066657529005 |\n",
      "|fineaggregate   |0.072124752236577   |\n",
      "|coarseaggregate |0.0657059587747203  |\n",
      "|flyash          |0.038410981510762644|\n",
      "+----------------+--------------------+\n",
      "\n",
      "None\n",
      "6.131978606195724\n"
     ]
    }
   ],
   "source": [
    "regressor = GBTRegressor()\n",
    "\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                           .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "                           .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "                           .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "GBT_BestModel = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each feature’s importance is the average of its importance across all trees \n",
    "# in the ensemble The importance vector is normalized to sum to 1. \n",
    "print(\" \")\n",
    "print('\\033[1m' + \" Feature Importances\"+ '\\033[0m')\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "GBT_featureImportances = GBT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in GBT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Which features are the strongest predictors of cement strength?\n",
    "\n",
    "Build your own ML model to figure this one out! This would be good information to give to our client so the sales reps can focus their efforts on certian ingredients to provide recommendations on. For example, if our clients had a customer that was struggling with their cement breaking, we could trouble shoot with them by starting with the factors that we know are important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For the following given inputs, what would be the estimated cement strength?\n",
    "\n",
    "- Cement: 540\n",
    "- Blast Furnace Slag: 0\n",
    "- Fly Ash: 0\n",
    "- Water: 162\n",
    "- Superplasticizer: 2.5\n",
    "- Coarse Aggregate: 1040\n",
    "- Fine Aggregate: 676\n",
    "- Age: 28\n",
    "\n",
    "The correct answer is 79.99. Let's how close your prediction is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[102.0, 153.0, 0.0, 192.0, 0.0, 887.0, 942.0, ...</td>\n",
       "      <td>25.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[108.3, 162.4, 0.0, 203.5, 0.0, 938.2, 849.0, ...</td>\n",
       "      <td>7.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[116.0, 173.0, 0.0, 192.0, 0.0, 909.8, 891.9, ...</td>\n",
       "      <td>22.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[116.0, 173.0, 0.0, 192.0, 0.0, 909.8, 891.9, ...</td>\n",
       "      <td>31.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[122.6, 183.9, 0.0, 203.5, 0.0, 958.2, 800.1, ...</td>\n",
       "      <td>10.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>[525.0, 0.0, 0.0, 189.0, 0.0, 1125.0, 613.0, 5...</td>\n",
       "      <td>61.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>[531.3, 0.0, 0.0, 141.8, 28.2, 852.1, 893.7, 2...</td>\n",
       "      <td>46.900002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>[531.3, 0.0, 0.0, 141.8, 28.2, 852.1, 893.7, 3...</td>\n",
       "      <td>56.400002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>[540.0, 0.0, 0.0, 173.0, 0.0, 1125.0, 613.0, 2...</td>\n",
       "      <td>59.759998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>[540.0, 0.0, 0.0, 173.0, 0.0, 1125.0, 613.0, 5...</td>\n",
       "      <td>74.169998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              features      label\n",
       "0    [102.0, 153.0, 0.0, 192.0, 0.0, 887.0, 942.0, ...  25.459999\n",
       "1    [108.3, 162.4, 0.0, 203.5, 0.0, 938.2, 849.0, ...   7.720000\n",
       "2    [116.0, 173.0, 0.0, 192.0, 0.0, 909.8, 891.9, ...  22.350000\n",
       "3    [116.0, 173.0, 0.0, 192.0, 0.0, 909.8, 891.9, ...  31.020000\n",
       "4    [122.6, 183.9, 0.0, 203.5, 0.0, 958.2, 800.1, ...  10.350000\n",
       "..                                                 ...        ...\n",
       "280  [525.0, 0.0, 0.0, 189.0, 0.0, 1125.0, 613.0, 5...  61.919998\n",
       "281  [531.3, 0.0, 0.0, 141.8, 28.2, 852.1, 893.7, 2...  46.900002\n",
       "282  [531.3, 0.0, 0.0, 141.8, 28.2, 852.1, 893.7, 3...  56.400002\n",
       "283  [540.0, 0.0, 0.0, 173.0, 0.0, 1125.0, 613.0, 2...  59.759998\n",
       "284  [540.0, 0.0, 0.0, 173.0, 0.0, 1125.0, 613.0, 5...  74.169998\n",
       "\n",
       "[285 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040</td>\n",
       "      <td>676</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement  slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0     540     0       0    162               2.5             1040   \n",
       "\n",
       "   fineaggregate  age  \n",
       "0            676   28  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_data = spark.createDataFrame([(540, 0, 0, 162, 2.5, 1040, 676, 28)],input_columns)\n",
    "inference_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = inference_data.withColumnRenamed(dependent_var,'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cement: bigint, slag: bigint, flyash: bigint, water: bigint, superplasticizer: double, coarseaggregate: bigint, fineaggregate: bigint, age: double]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_data.withColumn('age', log(\"age\")+1)\n",
    "\n",
    "## the same as the below\n",
    "# col = 'age'\n",
    "# indexed = indexed.withColumn(col, \\\n",
    "# log(when(inference_data[col] < d[col][0],d[col][0])\\\n",
    "# .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "# .otherwise(indexed[col] ) +1).alias(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[540.0,0.0,0.0,16...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "final_data = assembler.transform(inference_data).select('features')\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.07147479314204 72.26668717861176 65.69074065596969 288.7330045771338\n"
     ]
    }
   ],
   "source": [
    "print(GBT_BestModel.transform(final_data).toPandas().loc[0,'prediction'],\n",
    "RF_BestModel.transform(final_data).toPandas().loc[0,'prediction'],\n",
    "DT_BestModel.transform(final_data).toPandas().loc[0,'prediction'],\n",
    "LR_BestModel.transform(final_data).toPandas().loc[0,'prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Now see if you can ask users to input their own value for Age and return a predicted value for the cement stength. \n",
    "\n",
    "We did not cover this is in the lecture so you'll have to put your thinking cap on. Accepting user input in PySpark works just like it does in traditional Python.\n",
    "<br>\n",
    "\n",
    "val = input(\"Enter your value: \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your value: 28\n"
     ]
    }
   ],
   "source": [
    "val = input(\"Enter your value: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictCementStrength(age): \n",
    "    inference_data = spark.createDataFrame([(540, 0, 0, 162, 2.5, 1040, 676, float(age))],input_columns)\n",
    "    inference_data.withColumn('age', log(\"age\")+1)\n",
    "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "    final_data = assembler.transform(inference_data).select('features')\n",
    "    return RF_BestModel.transform(final_data).toPandas().loc[0,'prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.26668717861176"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictCementStrength(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make recommendations of optimal values for cement ingredients (our features)\n",
    "\n",
    "See if you can find the optimal amount of cement to recommend holding the rest of the values from the previous question constant, assuming that the higher the cement strength value the better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|summary|cement|csMPa|\n",
      "+-------+------+-----+\n",
      "|    min| 102.0| 2.33|\n",
      "|    max| 540.0| 82.6|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['cement', 'csMPa']).summary(['min', 'max']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
