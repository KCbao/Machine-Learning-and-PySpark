{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in Pyspark's MLlib\n",
    "\n",
    "Natural Language Processing (NLP) is a very trendy topic in the data science area today that is really handy for tasks like **chat bots**, movie or **product review analysis** and especially **tweet classification**. In this notebook, we will cover the **classification aspect of NLP** and go over the features that Spark has for cleaning and preparing your data for analysis. We will also touch on how to implement **ML Pipelines** to a few of our data processing steps to help make our code run a bit faster. \n",
    "\n",
    "As we learned in the NLP concept review lectures, the text you process must first be cleaned, tokenized and vectorized. Essentially, we need to covert our text into a vector of numbers. But how do we do that? Spark has a variety of built in functions to accomplish all of these tasks very easily. We will cover all of it here!\n",
    "\n",
    "### Agenda\n",
    "\n",
    "    1. Review Data (quality check)\n",
    "    2. Clean up the data (remove puncuation, special characters, etc.)\n",
    "    3. Tokenize text data\n",
    "    4. Remove Stopwords\n",
    "    5. Zero index our label column\n",
    "    5. Create an ML Pipeline (to streamline steps 3-5)\n",
    "    6. Vectorize Text column\n",
    "         - Count Vectors\n",
    "         - TF-IDF\n",
    "         - Word2Vec\n",
    "    7. Train and Evaluate Model (classification)\n",
    "    8. View Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://casiebaodembp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9193689dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"NLP\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import * #CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "from pyspark.sql.functions import * #col, udf,regexp_replace,isnull\n",
    "from pyspark.sql.types import * #StringType,IntegerType\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# For pipeline development\n",
    "from pyspark.ml import Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Dataset\n",
    "\n",
    "#### Kickstarter Dataset\n",
    "\n",
    "##### What is Kickstarter?\n",
    "\"Kickstarter is an American public-benefit corporation based in Brooklyn, New York, that maintains a global crowdfunding platform, focused on creativity and merchandising. The company's stated mission is to \"help bring creative projects to life\". Kickstarter, has reportedly received more than $1.9 billion in pledges from 9.4 million backers to fund 257,000 creative projects, such as films, music, stage shows, comics, journalism, video games, technology and food-related projects.\n",
    "\n",
    "People who back Kickstarter projects are offered tangible rewards or experiences in exchange for their pledges. This model traces its roots to subscription model of arts patronage, where artists would go directly to their audiences to fund their work\" ~ Wikipedia\n",
    "\n",
    "So, what if you can predict if a project will be or not to be able to get the money from their backers?\n",
    "\n",
    "#### Content\n",
    "\n",
    "The datastet contains the blurbs or short description of 215,513 projects runned along 2017, all written in english and all labeled with \"successful\" or \"failed\", if they get the money or not, respectively. From those texts you can train linguistics models for description, and even embeddings relative to the case.\n",
    "\n",
    "**Source:** https://www.kaggle.com/oscarvilla/kickstarter-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"Datasets/\"\n",
    "\n",
    "# CSV\n",
    "df = spark.read.csv(path+'kickstarter.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>blurb</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Using their own character, users go on educati...</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MicroFly is a quadcopter packed with WiFi, 6 s...</td>\n",
       "      <td>successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A small indie press, run as a collective for a...</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Zylor is a new baby cosplayer! Back this kicks...</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _c0                                              blurb       state\n",
       "0   1  Using their own character, users go on educati...      failed\n",
       "1   2  MicroFly is a quadcopter packed with WiFi, 6 s...  successful\n",
       "2   3  A small indie press, run as a collective for a...      failed\n",
       "3   4  Zylor is a new baby cosplayer! Back this kicks...      failed"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|_c0|blurb                                                                                                                              |state     |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|1  |Using their own character, users go on educational quests around a virtual world leveling up subject-oriented skills (ie Physics). |failed    |\n",
      "|2  |MicroFly is a quadcopter packed with WiFi, 6 sensors, and 3 processors for ultimate stability -- and fits in the palm of your hand.|successful|\n",
      "|3  |A small indie press, run as a collective for authors who want to self-publish, and a sexy, smart , hilarious novel!                |failed    |\n",
      "|4  |Zylor is a new baby cosplayer! Back this kickstarter to help fund new cosplay photoshoots to share his cuteness with the world!    |failed    |\n",
      "+---+-----------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's read a few full blurbs\n",
    "df.show(4,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the output above that the blurb text contains a good bit of punctuation and special characters. We'll need to clean that up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- blurb: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See how many rows are in the df?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many null values do we have?\n",
    "\n",
    "Let's use our handy dandy function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can not infer schema from empty dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-25c8fd3db42f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnull_columns_calc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnull_value_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnull_columns_calc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnull_columns_calc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Column_Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Null_Values_Count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Null_Value_Percent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Pyspark/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pyspark/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pyspark/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pyspark/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(df)\n",
    "print(null_columns_calc_list)\n",
    "spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! Less than 1% for the blurb column and about 5% of the state column. Unfortunatly though, we will need each row of data to contain value in both of these columns to conduct our analysis, so let's how many rows that actually effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows that contain at least one null value: 13157\n",
      "Percentage of Rows that contain at least one null value: 0.058834577220103115\n"
     ]
    }
   ],
   "source": [
    "# Of course you will want to know how many rows that affected before you actually execute it..\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop().count()\n",
    "print(\"Total Rows that contain at least one null value:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows that contain at least one null value:\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So dropping all rows that have at least one null value would impact just under 6% of our dataframe. I can live with that, so I'll go ahead and drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the null values\n",
    "# It's only about 6% so that's okay\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210470"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New df row count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to make this notebook run faster, you can slice the df like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced row count: 400\n"
     ]
    }
   ],
   "source": [
    "# Slice the dataframe down to make this notebook run faster\n",
    "df = df.limit(400)\n",
    "print('Sliced row count:',df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assurance Check (QA)\n",
    "\n",
    "Let's make sure our dependent variable column is clean before we go any further. This an important step in our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+-----+\n",
      "|state                                                                      |count|\n",
      "+---------------------------------------------------------------------------+-----+\n",
      "|failed                                                                     |240  |\n",
      "|successful                                                                 |156  |\n",
      "| best experienced with Oculus Rift.\"                                       |1    |\n",
      "| mixing pixel art graphics with deep storytelling and action combat system\"|1    |\n",
      "| smart                                                                     |1    |\n",
      "| Adventure with RPG elements.\"                                             |1    |\n",
      "+---------------------------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick data quality check on the state column....\n",
    "# This is going to be our category column so it's important\n",
    "df.groupBy(\"state\").count().orderBy(col(\"count\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the query above that we have some invalid data in the label (state) column. Let's delete those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|state     |count|\n",
      "+----------+-----+\n",
      "|failed    |240  |\n",
      "|successful|156  |\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(\"state IN('successful','failed')\")\n",
    "# Make sure it worked\n",
    "df.groupBy(\"state\").count().orderBy(col(\"count\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|blurb                                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Using their own character, users go on educational quests around a virtual world leveling up subject-oriented skills (ie Physics). |\n",
      "|MicroFly is a quadcopter packed with WiFi, 6 sensors, and 3 processors for ultimate stability -- and fits in the palm of your hand.|\n",
      "|A small indie press, run as a collective for authors who want to self-publish, and a sexy, smart , hilarious novel!                |\n",
      "|Zylor is a new baby cosplayer! Back this kickstarter to help fund new cosplay photoshoots to share his cuteness with the world!    |\n",
      "|Hatoful Boyfriend meet Skeletons! A comedy Dating Sim that puts you into a high school full of Skeletons. Rattle some Bones!       |\n",
      "|FastMan is a Infinite running platformer. Go in FastMan's shoes and run through the platform dodging obstacles.                    |\n",
      "|FADE. A dark and somber RPG about survival and hope.(Legend of Zelda/Fable Inspired)                                               |\n",
      "|The next generation of space combat with online progression, leveling, an arsenal of ships, weapons and much more!                 |\n",
      "|Whip around planets and smash your way to victory in this video game of galactic proportions!                                      |\n",
      "|Sneak in, find treasures, avoid cats and collect the loot before time runs out!                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the quality of the blurbs\n",
    "df.select(\"blurb\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some punctuation proper casing and some slashes which might making parsing problematic. Let's clean this up a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the blurb column\n",
    "\n",
    "Keep in mind that you can/should do all of this in one call...\n",
    "But we will show each individually for the purpose of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|blurb                                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Using their own character, users go on educational quests around a virtual world leveling up subject-oriented skills  ie Physics . |\n",
      "|MicroFly is a quadcopter packed with WiFi, 6 sensors, and 3 processors for ultimate stability -- and fits in the palm of your hand.|\n",
      "|A small indie press, run as a collective for authors who want to self-publish, and a sexy, smart , hilarious novel!                |\n",
      "|Zylor is a new baby cosplayer! Back this kickstarter to help fund new cosplay photoshoots to share his cuteness with the world!    |\n",
      "|Hatoful Boyfriend meet Skeletons! A comedy Dating Sim that puts you into a high school full of Skeletons. Rattle some Bones!       |\n",
      "|FastMan is a Infinite running platformer. Go in FastMan's shoes and run through the platform dodging obstacles.                    |\n",
      "|FADE. A dark and somber RPG about survival and hope. Legend of Zelda Fable Inspired                                                |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace Slashes and parenthesis with spaces\n",
    "# You can test your script on line 7 of the df \"(Legend of Zelda/Fable Inspired)\"\n",
    "df = df.withColumn(\"blurb\",translate(col(\"blurb\"), \"/\", \" \")) \\\n",
    "        .withColumn(\"blurb\",translate(col(\"blurb\"), \"(\", \" \")) \\\n",
    "        .withColumn(\"blurb\",translate(col(\"blurb\"), \")\", \" \"))\n",
    "df.select(\"blurb\").show(7,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|blurb                                                                                                                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Using their own character users go on educational quests around a virtual world leveling up subjectoriented skills  ie Physics |\n",
      "|MicroFly is a quadcopter packed with WiFi  sensors and  processors for ultimate stability  and fits in the palm of your hand   |\n",
      "|A small indie press run as a collective for authors who want to selfpublish and a sexy smart  hilarious novel                  |\n",
      "|Zylor is a new baby cosplayer Back this kickstarter to help fund new cosplay photoshoots to share his cuteness with the world  |\n",
      "|Hatoful Boyfriend meet Skeletons A comedy Dating Sim that puts you into a high school full of Skeletons Rattle some Bones      |\n",
      "|FastMan is a Infinite running platformer Go in FastMans shoes and run through the platform dodging obstacles                   |\n",
      "|FADE A dark and somber RPG about survival and hope Legend of Zelda Fable Inspired                                              |\n",
      "|The next generation of space combat with online progression leveling an arsenal of ships weapons and much more                 |\n",
      "|Whip around planets and smash your way to victory in this video game of galactic proportions                                   |\n",
      "|Sneak in find treasures avoid cats and collect the loot before time runs out                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing anything that is not a letter\n",
    "df = df.withColumn(\"blurb\",regexp_replace(col('blurb'), '[^A-Za-z ]+', ''))\n",
    "df.select(\"blurb\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|blurb                                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Using their own character users go on educational quests around a virtual world leveling up subjectoriented skills ie Physics |\n",
      "|MicroFly is a quadcopter packed with WiFi sensors and processors for ultimate stability and fits in the palm of your hand     |\n",
      "|A small indie press run as a collective for authors who want to selfpublish and a sexy smart hilarious novel                  |\n",
      "|Zylor is a new baby cosplayer Back this kickstarter to help fund new cosplay photoshoots to share his cuteness with the world |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove multiple spaces\n",
    "df = df.withColumn(\"blurb\",regexp_replace(col('blurb'), ' +', ' '))\n",
    "df.select(\"blurb\").show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|blurb                                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|using their own character users go on educational quests around a virtual world leveling up subjectoriented skills ie physics |\n",
      "|microfly is a quadcopter packed with wifi sensors and processors for ultimate stability and fits in the palm of your hand     |\n",
      "|a small indie press run as a collective for authors who want to selfpublish and a sexy smart hilarious novel                  |\n",
      "|zylor is a new baby cosplayer back this kickstarter to help fund new cosplay photoshoots to share his cuteness with the world |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lower case everything\n",
    "df = df.withColumn(\"blurb\",lower(col('blurb')))\n",
    "df.select(\"blurb\").show(4,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a pause here and go look at your Spark UI. You'll notice that only the \"show strings\" calls (as opposed to each of the data manipulation calls) are creating jobs. This is because of Sparks lazy computation. \n",
    "\n",
    "<br>\n",
    "So when you want to speed up your notebook those are some calls you can take out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data for NLP \n",
    "\n",
    "Alright so here is where our analysis turns from basic text cleaning to actually turning our text into number (the backbone of NLP). These next several steps in our analysis are very unique to NLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text into words (Tokenizing)\n",
    "\n",
    "Yo'll see a new column is added to our dataframe that we call \"words\". This column contains an array of strings as opposed to just a string (current data type of the blurb column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|blurb                                                                                                                         |state     |words                                                                                                                                            |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |using their own character users go on educational quests around a virtual world leveling up subjectoriented skills ie physics |failed    |[using, their, own, character, users, go, on, educational, quests, around, a, virtual, world, leveling, up, subjectoriented, skills, ie, physics]|\n",
      "|2  |microfly is a quadcopter packed with wifi sensors and processors for ultimate stability and fits in the palm of your hand     |successful|[microfly, is, a, quadcopter, packed, with, wifi, sensors, and, processors, for, ultimate, stability, and, fits, in, the, palm, of, your, hand]  |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- blurb: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"blurb\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "raw_words = regex_tokenizer.transform(df)\n",
    "raw_words.show(2,False)\n",
    "raw_words.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "**Recall from the content review lecture**\n",
    "Recall that \"stopwords\" are any word that we feel would \"distract\" our model from performing it's best. This list can be customized, but for now, we will just use the default list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Define a list of stop words or use default list\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "stopwords = remover.getStopWords() \n",
    "\n",
    "# Display default list\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|blurb                                                                                                                         |state |words                                                                                                                                            |filtered                                                                                                                  |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |using their own character users go on educational quests around a virtual world leveling up subjectoriented skills ie physics |failed|[using, their, own, character, users, go, on, educational, quests, around, a, virtual, world, leveling, up, subjectoriented, skills, ie, physics]|[using, character, users, go, educational, quests, around, virtual, world, leveling, subjectoriented, skills, ie, physics]|\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_df = remover.transform(raw_words)\n",
    "words_df.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to encode state column to a column of indices\n",
    "\n",
    "Remember that MLlib requres our dependent variable to not only be a numeric data type, but also zero indexed. We can Sparks handy built in StringIndexer function to accomplish this, just like we did in the classification lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+--------------------+--------------------+-----+\n",
      "|_c0|               blurb|     state|               words|            filtered|label|\n",
      "+---+--------------------+----------+--------------------+--------------------+-----+\n",
      "|  1|using their own c...|    failed|[using, their, ow...|[using, character...|  0.0|\n",
      "|  2|microfly is a qua...|successful|[microfly, is, a,...|[microfly, quadco...|  1.0|\n",
      "|  3|a small indie pre...|    failed|[a, small, indie,...|[small, indie, pr...|  0.0|\n",
      "|  4|zylor is a new ba...|    failed|[zylor, is, a, ne...|[zylor, new, baby...|  0.0|\n",
      "|  5|hatoful boyfriend...|    failed|[hatoful, boyfrie...|[hatoful, boyfrie...|  0.0|\n",
      "+---+--------------------+----------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- blurb: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"state\", outputCol=\"label\")\n",
    "feature_data = indexer.fit(words_df).transform(words_df)\n",
    "feature_data.show(5)\n",
    "feature_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat an ML Pipeline\n",
    "\n",
    "We could also create an ML Pipeline to accomplish the previous three steps in a more streamlined fashion. Pipelines allow users to combine any transformer call(s) and ONE estimator call in their ML workflow. Si a Pipeline can be a continuous set of transformer calls until you reach a point where you need to call \".fit()\" which is an estimator call. \n",
    "<br>\n",
    "\n",
    "Notice in the script below that we reduced our .transform calls from 3 to 1. So the benefit here is not necessarily speed but a bit less and more organized code (always nice) and little more streamlined. This feature can be esspecially useful when you get to the point where you want to move your model into production. You can save this pipeline to be called on whenever you need to prep new text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|_c0|blurb                                                                                                                         |state |words                                                                                                                                            |filtered                                                                                                                  |label|\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|1  |using their own character users go on educational quests around a virtual world leveling up subjectoriented skills ie physics |failed|[using, their, own, character, users, go, on, educational, quests, around, a, virtual, world, leveling, up, subjectoriented, skills, ie, physics]|[using, character, users, go, educational, quests, around, virtual, world, leveling, subjectoriented, skills, ie, physics]|0.0  |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################## BEFORE #############################\n",
    "# Tokenize\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"blurb\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "raw_words = regex_tokenizer.transform(df)\n",
    "\n",
    "# Remove Stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "words_df = remover.transform(raw_words)\n",
    "\n",
    "# Zero Index Label Column\n",
    "indexer = StringIndexer(inputCol=\"state\", outputCol=\"label\")\n",
    "feature_data = indexer.fit(words_df).transform(words_df)\n",
    "\n",
    "feature_data.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|_c0|blurb                                                                                                                         |state |words                                                                                                                                            |filtered                                                                                                                  |label|\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|1  |using their own character users go on educational quests around a virtual world leveling up subjectoriented skills ie physics |failed|[using, their, own, character, users, go, on, educational, quests, around, a, virtual, world, leveling, up, subjectoriented, skills, ie, physics]|[using, character, users, go, educational, quests, around, virtual, world, leveling, subjectoriented, skills, ie, physics]|0.0  |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------+------+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# AFTER ##################\n",
    "\n",
    "# Tokenize\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"blurb\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# raw_words = regex_tokenizer.transform(df)\n",
    "\n",
    "# Remove Stop words\n",
    "remover = StopWordsRemover(inputCol=regex_tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "# words_df = remover.transform(raw_words)\n",
    "\n",
    "# Zero Index Label Column\n",
    "indexer = StringIndexer(inputCol=\"state\", outputCol=\"label\")\n",
    "# feature_data = indexer.fit(words_df).transform(words_df)\n",
    "\n",
    "# Create the Pipeline\n",
    "pipeline = Pipeline(stages=[regex_tokenizer,remover,indexer])\n",
    "data_prep_pl = pipeline.fit(df)\n",
    "# print(type(data_prep_pl))\n",
    "# print(\" \")\n",
    "# Now call on the Pipeline to get our final df\n",
    "feature_data = data_prep_pl.transform(df)\n",
    "feature_data.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the Spark UI again. You'll see the last 2 \"countbyvalue\" job ids for each one of these. If you take a look at how long it took each of those job ids to run, you will see that the second job id actually took just a bit less time to run. Since we do not have much data here it only saved us .2 seconds but that may translate to a couple of miniutes on a much larger df. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text into vectors\n",
    "\n",
    "We will test out the following three vectorizors:\n",
    "\n",
    "1. Count Vectors\n",
    "2. TF-IDF\n",
    "3. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vector (count vectorizer and hashingTF are basically the same thing)\n",
    "# cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "# model = cv.fit(feature_data)\n",
    "# countVectorizer_features = model.transform(feature_data)\n",
    "\n",
    "# Hashing TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawfeatures\", numFeatures=20)\n",
    "HTFfeaturizedData = hashingTF.transform(feature_data)\n",
    "\n",
    "# TF-IDF\n",
    "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(HTFfeaturizedData)\n",
    "TFIDFfeaturizedData = idfModel.transform(HTFfeaturizedData)\n",
    "TFIDFfeaturizedData.name = 'TFIDFfeaturizedData'\n",
    "\n",
    "#rename the HTF features to features to be consistent\n",
    "HTFfeaturizedData = HTFfeaturizedData.withColumnRenamed(\"rawfeatures\",\"features\")\n",
    "HTFfeaturizedData.name = 'HTFfeaturizedData' #We will use later for printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered\", outputCol=\"features\")\n",
    "model = word2Vec.fit(feature_data)\n",
    "\n",
    "W2VfeaturizedData = model.transform(feature_data)\n",
    "# W2VfeaturizedData.show(1,False)\n",
    "\n",
    "# W2Vec Dataframes typically has negative values so we will correct for that here so that we can use the Naive Bayes classifier\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(W2VfeaturizedData)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(W2VfeaturizedData)\n",
    "W2VfeaturizedData = scaled_data.select('state','blurb','label','scaledFeatures')\n",
    "W2VfeaturizedData = W2VfeaturizedData.withColumnRenamed('scaledFeatures','features')\n",
    "\n",
    "W2VfeaturizedData.name = 'W2VfeaturizedData' # We will need this to print later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate your model\n",
    "\n",
    "From here on out, is straight up classification. So we can go and use our trusty function! I'll just go ahead and copy and paste it in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassTrainEval(classifier,features,classes,train,test):\n",
    "\n",
    "    def FindMtype(classifier):\n",
    "        # Intstantiate Model\n",
    "        M = classifier\n",
    "        # Learn what it is\n",
    "        Mtype = type(M).__name__\n",
    "        \n",
    "        return Mtype\n",
    "    \n",
    "    Mtype = FindMtype(classifier)\n",
    "    \n",
    "\n",
    "    def IntanceFitModel(Mtype,classifier,classes,features,train):\n",
    "        \n",
    "        if Mtype == \"OneVsRest\":\n",
    "            # instantiate the base classifier.\n",
    "            lr = LogisticRegression()\n",
    "            # instantiate the One Vs Rest Classifier.\n",
    "            OVRclassifier = OneVsRest(classifier=lr)\n",
    "#             fitModel = OVRclassifier.fit(train)\n",
    "            # Add parameters of your choice here:\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "            #Cross Validator requires the following parameters:\n",
    "            crossval = CrossValidator(estimator=OVRclassifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2) # 3 is best practice\n",
    "            # Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            # specify layers for the neural network:\n",
    "            # input layer of size features, two intermediate of features+1 and same size as features\n",
    "            # and output of size number of classes\n",
    "            # Note: crossvalidator cannot be used here\n",
    "            features_count = len(features[0][0])\n",
    "            layers = [features_count, features_count+1, features_count, classes]\n",
    "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "            fitModel = MPC_classifier.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2: # These classifiers currently only accept binary classification\n",
    "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
    "            return\n",
    "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
    "  \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LogisticRegression\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,20])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"NaiveBayes\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LinearSVC\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .build())\n",
    "            \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .build())\n",
    "            \n",
    "            #Cross Validator requires all of the following parameters:\n",
    "            crossval = CrossValidator(estimator=classifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2) # 3 + is best practice\n",
    "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "    \n",
    "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,train)\n",
    "    \n",
    "    # Print feature selection metrics\n",
    "    if fitModel is not None:\n",
    "        \n",
    "        if Mtype in(\"OneVsRest\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            # Extract list of binary models\n",
    "            models = BestModel.models\n",
    "            for model in models:\n",
    "                print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept,'\\033[1m' + '\\nCoefficients:'+ '\\033[0m',model.coefficients)\n",
    "\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            print(\"\")\n",
    "            print('\\033[1m' + Mtype,\" Weights\"+ '\\033[0m')\n",
    "            print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
    "            print(\"\")\n",
    "\n",
    "        if Mtype in(\"DecisionTreeClassifier\", \"GBTClassifier\",\"RandomForestClassifier\"):\n",
    "            # FEATURE IMPORTANCES\n",
    "            # Estimate of the importance of each feature.\n",
    "            # Each features importance is the average of its importance across all trees \n",
    "            # in the ensemble The importance vector is normalized to sum to 1. \n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Feature Importances\"+ '\\033[0m')\n",
    "            print(\"(Scores add up to 1)\")\n",
    "            print(\"Lowest score is the least important\")\n",
    "            print(\" \")\n",
    "            print(BestModel.featureImportances)\n",
    "            \n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                global DT_featureimportances\n",
    "                DT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global DT_BestModel\n",
    "                DT_BestModel = BestModel\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                global GBT_featureimportances\n",
    "                GBT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global GBT_BestModel\n",
    "                GBT_BestModel = BestModel\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                global RF_featureimportances\n",
    "                RF_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global RF_BestModel\n",
    "                RF_BestModel = BestModel\n",
    "\n",
    "        if Mtype in(\"LogisticRegression\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Coefficient Matrix\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "            print(\"Coefficients: \\n\" + str(BestModel.coefficientMatrix))\n",
    "            print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "            global LR_coefficients\n",
    "            LR_coefficients = BestModel.coefficientMatrix.toArray()\n",
    "            global LR_BestModel\n",
    "            LR_BestModel = BestModel\n",
    "\n",
    "        if Mtype in(\"LinearSVC\"):\n",
    "            # Get Best Model\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Coefficients\"+ '\\033[0m')\n",
    "            print(\"You should compares these relative to eachother\")\n",
    "            print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
    "            global LSVC_coefficients\n",
    "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
    "            global LSVC_BestModel\n",
    "            LSVC_BestModel = BestModel\n",
    "        \n",
    "   \n",
    "    # Set the column names to match the external results dataframe that we will join with later:\n",
    "    columns = ['Classifier', 'Result']\n",
    "    \n",
    "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
    "        Mtype = [Mtype] # make this a list\n",
    "        score = [\"N/A\"]\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "    else:\n",
    "        predictions = fitModel.transform(test)\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        Mtype = [Mtype] # make this a string\n",
    "        score = [str(accuracy)] #make this a string and convert to a list\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        \n",
    "    return result\n",
    "    #Also returns the fit model important scores or p values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the algorithims you want to test plus declare a list of all the different feature vectors we want to test out that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import *\n",
    "# from pyspark.ml.evaluation import *\n",
    "# from pyspark.sql import functions\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Comment out Naive Bayes if your data still contains negative values\n",
    "classifiers = [\n",
    "                LogisticRegression()\n",
    "                ,OneVsRest()\n",
    "               ,LinearSVC()\n",
    "               ,NaiveBayes()\n",
    "               ,RandomForestClassifier()\n",
    "               ,GBTClassifier()\n",
    "               ,DecisionTreeClassifier()\n",
    "               ,MultilayerPerceptronClassifier()\n",
    "              ] \n",
    "\n",
    "featureDF_list = [HTFfeaturizedData,TFIDFfeaturizedData,W2VfeaturizedData]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all feature types (hashingTF, TFIDF and Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHTFfeaturizedData  Results:\u001b[0m\n",
      " \n",
      "\u001b[1mLogisticRegression  Coefficient Matrix\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "DenseMatrix([[-0.0532213 ,  0.23415045, -0.07400952,  0.34757327,  0.17108948,\n",
      "               0.04331952, -0.01367361, -0.04603223,  0.00267074,  0.03660453,\n",
      "              -0.2486708 , -0.09634587,  0.02361933, -0.38839715, -0.09499649,\n",
      "              -0.3640405 ,  0.14341329,  0.13233158,  0.12517172, -0.23841872]])\n",
      "Intercept: [-0.07486047443573277]\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m 0.10858209857917128 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.052925186314513414,-0.22178824089067428,0.06334754973392837,-0.33018382185334416,-0.15786038538733801,-0.04685761726720413,0.003670063771401054,0.04111205638052649,-0.012344736232826127,-0.03347890397803068,0.23614971503947318,0.08731760069275574,-0.012784430616861818,0.36134070194370516,0.08696494941659878,0.344097699839436,-0.13628437823517006,-0.12492809088767579,-0.11276278773997092,0.22689507332302858]\n",
      "\u001b[1mIntercept: \u001b[0m -0.10858209857910829 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.05292518631456381,0.22178824089069532,-0.063347549733947,0.3301838218534121,0.15786038538735228,0.04685761726717471,-0.003670063771425098,-0.04111205638055372,0.012344736232793977,0.03347890397801816,-0.2361497150394941,-0.08731760069280256,0.01278443061687457,-0.3613407019437011,-0.08696494941658438,-0.3440976998394473,0.13628437823517456,0.1249280908876596,0.11276278773996519,-0.22689507332303796]\n",
      " \n",
      "\u001b[1mLinearSVC  Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "[-0.07839115123349318,0.08880386964265485,-0.08680410719766173,0.2273618907012781,0.08440590712959195,-0.03612263411185646,-0.05000435422031213,-0.1363205818569438,-0.061427570452961384,-0.07256330890405317,-0.11459285817238266,-0.05393410241743166,-0.002669330253993599,-0.2581926025522198,-0.07739510278756567,-0.222994567734667,0.028096146720181164,0.1276513814033395,0.0913189200261588,-0.1637494896689602]\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.07960349456819127,0.03843104366148621,0.02617285105581051,0.04847151720968789,0.057605167264652,0.04345037600957579,0.043119709595066065,0.08081538687931315,0.05074037713551746,0.03287109586400448,0.045248704430052716,0.032718250033066096,0.05722200308293005,0.0653863457969654,0.044583800111093114,0.09488500542154639,0.03317205162218944,0.05260878224274212,0.04451394120025016,0.028380096815859464])\n",
      " \n",
      "\u001b[1mGBTClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.05556946041748907,0.049465841026737296,0.04575645357218589,0.04425476462401333,0.05745492752664355,0.05118323385017868,0.03361188609065979,0.028267908274750307,0.05571454840849118,0.051104762529407315,0.04586481812771528,0.0537038328643422,0.11497650231974958,0.0375458210579901,0.045759122224597513,0.05350452887518805,0.027233464372706996,0.057819674211945456,0.05140733460000084,0.03980111502520746])\n",
      " \n",
      "\u001b[1mDecisionTreeClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,2,3,4,5,6,7,8,9,10,11,12,13,15,17,19],[0.0476876383277113,0.027566858269252655,0.0780905962392416,0.08618321341526569,0.02289006639730143,0.042969968393148476,0.042138531322304905,0.09258278969786106,0.06564063158049677,0.07415601943702996,0.0515026493939282,0.04226858851774412,0.07158561924139231,0.09051595465555712,0.11116182697241517,0.05305904813934906])\n",
      "\n",
      "\u001b[1mMultilayerPerceptronClassifier  Weights\u001b[0m\n",
      "\u001b[1mModel Weights: \u001b[0m 923\n",
      "\n",
      "+------------------------------+------+\n",
      "|Classifier                    |Result|\n",
      "+------------------------------+------+\n",
      "|LogisticRegression            |64.22 |\n",
      "|OneVsRest                     |64.22 |\n",
      "|LinearSVC                     |61.46 |\n",
      "|NaiveBayes                    |64.22 |\n",
      "|RandomForestClassifier        |66.05 |\n",
      "|GBTClassifier                 |53.21 |\n",
      "|DecisionTreeClassifier        |61.46 |\n",
      "|MultilayerPerceptronClassifier|57.79 |\n",
      "+------------------------------+------+\n",
      "\n",
      "None\n",
      "\u001b[1mTFIDFfeaturizedData  Results:\u001b[0m\n",
      " \n",
      "\u001b[1mLogisticRegression  Coefficient Matrix\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "DenseMatrix([[-0.07706179,  0.22007545, -0.08429418,  0.51820123,  0.2163008 ,\n",
      "               0.04669609, -0.01443911, -0.05100344,  0.00349766,  0.0437622 ,\n",
      "              -0.27934698, -0.10529234,  0.02784846, -0.65291696, -0.13171161,\n",
      "              -0.66163404,  0.15781163,  0.15820776,  0.12860536, -0.28306594]])\n",
      "Intercept: [-0.07486047443572927]\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m 0.10858209857921293 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.0766330375738604,-0.20845634159155021,0.07215058431621185,-0.4922750933472184,-0.19957584159659636,-0.05050996912532373,0.0038755301232609885,0.04555191419297057,-0.016166909645433564,-0.040025383503061146,0.26528129385170696,0.09542572631881463,-0.01507352927683611,0.6074335769627247,0.12057597317679122,0.6253885334136517,-0.1499669973354119,-0.14935658441402402,-0.11585602942306193,0.26938432981157906]\n",
      "\u001b[1mIntercept: \u001b[0m -0.10858209857919028 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.07663303757388543,0.20845634159155702,-0.072150584316219,0.49227509334725256,0.19957584159660258,0.05050996912531287,-0.0038755301232693924,-0.04555191419298082,0.0161669096454191,0.04002538350305637,-0.26528129385171506,-0.09542572631883219,0.015073529276840572,-0.6074335769627229,-0.12057597317678463,-0.6253885334136589,0.14996699733541316,0.14935658441401767,0.11585602942305989,-0.26938432981158283]\n",
      " \n",
      "\u001b[1mLinearSVC  Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "[-0.11350648823868133,0.08346578569964358,-0.09886676093495686,0.3389766201758359,0.10671062223610084,-0.03893823971692808,-0.052803818447509795,-0.1510423947177559,-0.08044675580909173,-0.08675237006089052,-0.12872910592780212,-0.05894230780170314,-0.0031472835153151307,-0.43403595351968655,-0.10730748307600646,-0.4052867709951917,0.03091693130851811,0.1526123883480707,0.09382392629232882,-0.1944138578481196]\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.07960349456819127,0.03843104366148621,0.02617285105581051,0.04847151720968789,0.057605167264652,0.04345037600957579,0.043119709595066065,0.08081538687931315,0.05074037713551746,0.03287109586400448,0.045248704430052716,0.032718250033066096,0.05722200308293005,0.0653863457969654,0.044583800111093114,0.09488500542154639,0.03317205162218944,0.05260878224274212,0.04451394120025016,0.028380096815859464])\n",
      " \n",
      "\u001b[1mGBTClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.05556946041748907,0.049465841026737296,0.04575645357218589,0.04425476462401333,0.05745492752664355,0.05118323385017868,0.03361188609065979,0.028267908274750307,0.05571454840849118,0.051104762529407315,0.04586481812771528,0.0537038328643422,0.11497650231974958,0.0375458210579901,0.045759122224597513,0.05350452887518805,0.027233464372706996,0.057819674211945456,0.05140733460000084,0.03980111502520746])\n",
      " \n",
      "\u001b[1mDecisionTreeClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(20,[0,2,3,4,5,6,7,8,9,10,11,12,13,15,17,19],[0.0476876383277113,0.027566858269252655,0.0780905962392416,0.08618321341526569,0.02289006639730143,0.042969968393148476,0.042138531322304905,0.09258278969786106,0.06564063158049677,0.07415601943702996,0.0515026493939282,0.04226858851774412,0.07158561924139231,0.09051595465555712,0.11116182697241517,0.05305904813934906])\n",
      "\n",
      "\u001b[1mMultilayerPerceptronClassifier  Weights\u001b[0m\n",
      "\u001b[1mModel Weights: \u001b[0m 923\n",
      "\n",
      "+------------------------------+------+\n",
      "|Classifier                    |Result|\n",
      "+------------------------------+------+\n",
      "|LogisticRegression            |64.22 |\n",
      "|OneVsRest                     |64.22 |\n",
      "|LinearSVC                     |61.46 |\n",
      "|NaiveBayes                    |61.46 |\n",
      "|RandomForestClassifier        |66.05 |\n",
      "|GBTClassifier                 |53.21 |\n",
      "|DecisionTreeClassifier        |61.46 |\n",
      "|MultilayerPerceptronClassifier|53.21 |\n",
      "+------------------------------+------+\n",
      "\n",
      "None\n",
      "\u001b[1mW2VfeaturizedData  Results:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1mLogisticRegression  Coefficient Matrix\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "DenseMatrix([[-1.9427975 , -1.99938328,  3.59946904]])\n",
      "\n",
      "Intercept: [-0.6717029394132809]\n",
      " \n",
      "\u001b[1mOneVsRest\u001b[0m\n",
      "\u001b[1mIntercept: \u001b[0m 1.56577252228525 \u001b[1m\n",
      "Coefficients:\u001b[0m [1.6358837402339885,1.2719737265951847,-4.1982670552889285]\n",
      "\u001b[1mIntercept: \u001b[0m -1.5657725222852077 \u001b[1m\n",
      "Coefficients:\u001b[0m [-1.635883740234005,-1.2719737265951832,4.198267055288888]\n",
      " \n",
      "\u001b[1mLinearSVC  Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "[-0.7102636458226547,-1.7235487054764278,0.7864412862378372]\n",
      " \n",
      "\u001b[1mRandomForestClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(3,[0,1,2],[0.27617394032112075,0.3194660675229377,0.4043599921559415])\n",
      " \n",
      "\u001b[1mGBTClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(3,[0,1,2],[0.3732621235255516,0.31824867248196964,0.30848920399247887])\n",
      " \n",
      "\u001b[1mDecisionTreeClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(3,[0,1,2],[0.2356673238306887,0.3015645886215527,0.46276808754775867])\n",
      "\n",
      "\u001b[1mMultilayerPerceptronClassifier  Weights\u001b[0m\n",
      "\u001b[1mModel Weights: \u001b[0m 39\n",
      "\n",
      "+------------------------------+------+\n",
      "|Classifier                    |Result|\n",
      "+------------------------------+------+\n",
      "|LogisticRegression            |68.80 |\n",
      "|OneVsRest                     |69.72 |\n",
      "|LinearSVC                     |62.38 |\n",
      "|NaiveBayes                    |62.38 |\n",
      "|RandomForestClassifier        |64.22 |\n",
      "|GBTClassifier                 |59.63 |\n",
      "|DecisionTreeClassifier        |61.46 |\n",
      "|MultilayerPerceptronClassifier|40.36 |\n",
      "+------------------------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for featureDF in featureDF_list:\n",
    "    print('\\033[1m' + featureDF.name,\" Results:\"+ '\\033[0m')\n",
    "    train, test = featureDF.randomSplit([0.7, 0.3],seed = 11)\n",
    "    features = featureDF.select(['features']).collect()\n",
    "    # Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
    "    class_count = featureDF.select(countDistinct(\"label\")).collect()\n",
    "    classes = class_count[0][0]\n",
    "\n",
    "    #set up your results table\n",
    "    columns = ['Classifier', 'Result']\n",
    "    vals = [(\"Place Holder\",\"N/A\")]\n",
    "    results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        new_result = ClassTrainEval(classifier,features,classes,train,test)\n",
    "        results = results.union(new_result)\n",
    "    results = results.where(\"Classifier!='Place Holder'\")\n",
    "    print(results.show(truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the Decision Tree classifier with the W2VfeaturizedData was our best performing feature list/classifier combo. Let's go with that and create our final model and play around with the test dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1mDecisionTreeClassifier  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "(3,[0,1,2],[0.2808481209180027,0.16992749361121173,0.5492243854707857])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Classifier: string, Result: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "featureDF = W2VfeaturizedData\n",
    "\n",
    "train, test = featureDF.randomSplit([0.7, 0.3],seed = 11)\n",
    "features = featureDF.select(['features']).collect()\n",
    "\n",
    "# Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
    "class_count = featureDF.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "\n",
    "#running this afain with generate all the objects need to play around with test data\n",
    "ClassTrainEval(classifier,features,classes,train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Failures:\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|state |blurb                                                                                                                          |\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "|failed|a book about abuse and how to remove ones self from an abusive situation                                                       |\n",
      "|failed|a collection of essays about life as an alien artistponderer in the iconic city of angels                                      |\n",
      "|failed|a complete game and a great learning opportunity for aspiring web game developers learn how to build it and have fun playing it|\n",
      "+------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      " \n",
      "Predicted Success:\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|state |blurb                                                                                                                             |\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|failed| yr old spirit of of daniel reveals antichrist horses apocalypse iraq war ot prophecy revelation into current events              |\n",
      "|failed|a point n click game mixing up hard boiled detective stories lovecraftian horrors and dark humor in an intriguing mysterious story|\n",
      "|failed|a style experience that will display style in several forms fashion art and music this is style chronicles                        |\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = DT_BestModel.transform(test)\n",
    "print(\"Predicted Failures:\")\n",
    "predictions.select(\"state\",\"blurb\").filter(\"prediction=0\").orderBy(predictions[\"prediction\"].desc()).show(3,False)\n",
    "print(\" \")\n",
    "print(\"Predicted Success:\")\n",
    "predictions.select(\"state\",\"blurb\").filter(\"prediction=1\").orderBy(predictions[\"prediction\"].desc()).show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could be next?\n",
    "\n",
    "Once we have our model and all the vectorizer the sky is really the limit! We could do any of the following for starters:\n",
    "\n",
    "1. Allow a user to input their own \"blurb\" and we could return a prediction of whether or not it would pass\n",
    "2. If we had a time variable here, we could show the most popular words over time\n",
    "3. Provide this algorithim to Kickstarter for prescreening so they can prioritize entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
