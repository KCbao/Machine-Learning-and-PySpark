{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in PySpark's MLlib\n",
    "\n",
    "PySpark offers 7 algorithms for regression which we will review in this lecture. The content of this notebook will be very similar to what we did in the classification lectures where we will see how to prep our data first, and then go over how to train and evaluate each model individually. \n",
    "\n",
    "**Recap from the Regression lecture**<br>\n",
    "Remember that regression problems require that the **dependent variable** in your dataset be continuous (like age or height) and not categorical like (young vs old, or fat vs skinny). Regression analysis tries to find the relationship between this variable (the dependent) and each of the independent variables which can be either continous or categorical. As with any machine learning problem, the basic question of regression analysis is \"what factors affect our outcome.\"\n",
    "\n",
    "For example, some research questions that might be solved using regression anlaysis might be:\n",
    "\n",
    "What factors effect...\n",
    "\n",
    "1. inflation rate and how we predict it longer term?\n",
    "2. price increases upon demand\n",
    "3. the height or weight of a person\n",
    "4. crop yield of vegetation like corn or apple trees\n",
    "5. income of a person\n",
    "\n",
    "\n",
    "## Available Algorithms\n",
    "These are the regression algorithms Spark offers:\n",
    "\n",
    "1. Linear regression \n",
    "     - most simplistic and easy to understand\n",
    "2. Generalized linear regression (out of scope)\n",
    "3. Decision tree regression \n",
    "     - most basic of the tree algorithms)\n",
    "4. Random forest regression \n",
    "     - a bit more complex than decision tree as it is an ensemble method as it combines several models in order to produce one really great predictive model\n",
    "5. Gradient-boosted tree regression \n",
    "     - most complex of the tree algorithms as it takes a more hierarchical approach to learning making it more efficient\n",
    "6. Survival regression (Out of scope)\n",
    "7. Isotonic regression (Out of scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.53:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Regression</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd54ea63150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Regression\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "\n",
    "This is a dataset containing housing pricing data for California. Each row of data represents the median statistics for a block (eg. median income, median age of house, etc.). You could this data in a number of ways, but we will use it to predict the median house value. \n",
    "\n",
    "\n",
    "### About this dataset \n",
    "\n",
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "6. population: Total number of people residing within a block\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea\n",
    "\n",
    "**Source:** https://www.kaggle.com/camnugent/california-housing-prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"Datasets/\"\n",
    "df = spark.read.csv(path+'housing.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>4.0368</td>\n",
       "      <td>269700.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "5    -122.25     37.85                52.0        919.0           213.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
       "5       413.0       193.0         4.0368            269700.0        NEAR BAY  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And of course the schema :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Starting\n",
    "print(df.count())\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cut this down to a smaller dataframe size to get results faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to make your code run faster, you can slice the df like this...\n",
    "# # Slice rows\n",
    "# df = df.limit(300)\n",
    "\n",
    "# # Slice columns\n",
    "# cols_list = df.columns[4:9]\n",
    "# df = df.select(cols_list)\n",
    "\n",
    "# # QA\n",
    "# print(df.count())\n",
    "# print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop any missing values**\n",
    "\n",
    "Let's go ahead and drop any missing values for the sake of simplicity for this lecture as we have already covered the alternatives in subsequent lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20433"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop missing data\n",
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data \n",
    "\n",
    "**MLlib requires all input columns of your dataframe to be vectorized.** You will see that we rename our dependent var to label as that is what is expected for all MLlib applications. If we rename once here, we won't need to specify it later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLRegressDFPrep(df,input_columns,dependent_var,treat_outliers=True):\n",
    "\n",
    "    renamed = df.withColumnRenamed(dependent_var,'label')\n",
    "    \n",
    "    # Make sure dependent variable is numeric and change if it's not\n",
    "    if str(renamed.schema['label'].dataType) != 'IntegerType':\n",
    "        renamed = renamed.withColumn(\"label\", renamed[\"label\"].cast(FloatType()))\n",
    "    \n",
    "   # Convert all string type data in the input column list to numeric\n",
    "    # Otherwise the Algorithm will not be able to process it\n",
    "    numeric_inputs = []\n",
    "    string_inputs = []\n",
    "    for column in input_columns:\n",
    "        if str(renamed.schema[column].dataType) == 'StringType':\n",
    "            new_col_name = column+\"_num\"\n",
    "            string_inputs.append(new_col_name)\n",
    "        else:\n",
    "            numeric_inputs.append(column)\n",
    "            indexed = renamed\n",
    "            \n",
    "    if len(string_inputs) != 0: # If the datafraem contains string types\n",
    "        for column in input_columns:\n",
    "            if str(renamed.schema[column].dataType) == 'StringType':\n",
    "                indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "                indexed = indexer.fit(renamed).transform(renamed)\n",
    "    else:\n",
    "        indexed = renamed\n",
    "        \n",
    "            \n",
    "    if treat_outliers == True:\n",
    "        print(\"We are correcting for non normality now!\")\n",
    "        # empty dictionary d\n",
    "        d = {}\n",
    "        # Create a dictionary of quantiles\n",
    "        for col in numeric_inputs: \n",
    "            d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "        #Now fill in the values\n",
    "        for col in numeric_inputs:\n",
    "            skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "            skew = skew[0][0]\n",
    "            # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "            if skew > 1:\n",
    "                indexed = indexed.withColumn(col, \\\n",
    "                log(when(df[col] < d[col][0],d[col][0])\\\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(indexed[col] ) +1).alias(col))\n",
    "                print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "            elif skew < -1:\n",
    "                indexed = indexed.withColumn(col, \\\n",
    "                exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "                .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                .otherwise(indexed[col] )).alias(col))\n",
    "                print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
    "                \n",
    "    # Vectorize your features\n",
    "    features_list = numeric_inputs + string_inputs\n",
    "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "    final_data = assembler.transform(indexed).select('features','label')\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are correcting for non normality now!\n",
      "total_bedrooms has been treated for positive (right) skewness. (skew =) 3.4592923587675024 )\n",
      "population has been treated for positive (right) skewness. (skew =) 4.959652416875933 )\n",
      "households has been treated for positive (right) skewness. (skew =) 3.4135995729616138 )\n",
      "median_income has been treated for positive (right) skewness. (skew =) 1.6444361858367003 )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4.867534450455582, 5.777652323222656, 4.84418...</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features     label\n",
       "0  [4.867534450455582, 5.777652323222656, 4.84418...  452600.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "input_columns = ['total_bedrooms','population','households','median_income']\n",
    "dependent_var = 'median_house_value'\n",
    "\n",
    "final_data = MLRegressDFPrep(df,input_columns,dependent_var)\n",
    "final_data.toPandas().head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Multicollinearity\n",
    "\n",
    "Multicollinearity generally occurs when there are high correlations between two or more predictor variables (your features column in your dataframe, also called independent variables). In other words, one predictor variable can be used to predict the other. This creates redundant information, skewing the results in a regression model. \n",
    "\n",
    "An easy way to detect multicollinearity is to calculate correlation coefficients for all pairs of predictor variables. If the correlation coefficient, is exactly +1 or -1, this is called perfect multicollinearity, and one of the variables should be removed from the model if at all possible for the linear model to perform well.\n",
    "\n",
    "Desicion trees on the other hand, make no assumptions on relationships between features. It just constructs splits on single features that improves classification, based on an impurity measure like Gini or entropy. If features A, B are heavily correlated, no /little information can be gained from splitting on B after having split on A. So it would typically get ignored in favor of C.\n",
    "\n",
    "Of course a single decision tree is very vulnerable to overfitting, so one must either limit depth, prune heavily or preferly average many using an ensemble. Such problems get worse with many features and possibly also with co-variance but this problem occurs independently from multicolinearity.\n",
    "\n",
    "MLlib offers two correlation coefficient statitics: **pearson** and **spearman**. \n",
    "\n",
    "**Sources:**\n",
    "\n",
    " - https://datascience.stackexchange.com/questions/31402/multicollinearity-in-decision-tree\n",
    " - https://www.statisticshowto.datasciencecentral.com/multicollinearity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "pearsonCorr = Correlation.corr(final_data, 'features', 'pearson').collect()[0][0]\n",
    "array = pearsonCorr.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.89752321 0.97459262 0.0144038 ]\n",
      "[0.89752321 1.         0.93218972 0.03178967]\n",
      "[0.97459262 0.93218972 1.         0.04694401]\n",
      "[0.0144038  0.03178967 0.04694401 1.        ]\n"
     ]
    }
   ],
   "source": [
    "for item in array:\n",
    "    print(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the first and second features are highly correlated, along with the 4th and 5th and 4th and 6th. We may want to consider removing one of the variables in each correlation pair if we decide to use a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split dataframe into training and evaluation (test) dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies for this section\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Simplistic Training method\n",
    "\n",
    "Let's train a Linear Regression algorithm to start with just to show the most simlictic way to train and test a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model\n",
    "regressor = LinearRegression()\n",
    "fitModel = regressor.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Evaluation/Test method for regression (rmse/r^2)\n",
    "\n",
    "We will use the Root Mean Squared Error as our evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 82658.5\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = fitModel.transform(test)\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function that iterativley runs through all Regression algorithms\n",
    "\n",
    "**Note:**\n",
    "We did not include Generalized Linear Regression here since it requires a much different implementation method and evaluation strategy than most regressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressTrainEval(regressor):\n",
    "\n",
    "    def FindMtype(regressor):\n",
    "        # Intstantiate Model\n",
    "        M = regressor\n",
    "        # Learn what it is\n",
    "        Mtype = type(M).__name__\n",
    "        \n",
    "        return Mtype\n",
    "    \n",
    "    Mtype = FindMtype(regressor)\n",
    "#     print('\\033[1m' + Mtype + ':' + '\\033[0m')\n",
    "\n",
    "\n",
    "    if Mtype == \"LinearRegression\":\n",
    "        \n",
    "        #first without cross val\n",
    "        fitModel = regressor.fit(train)\n",
    "\n",
    "        # Load the Summary\n",
    "        trainingSummary = fitModel.summary\n",
    "        \n",
    "        # Print the coefficients and intercept for linear regression\n",
    "        print('\\033[1m' + \"Linear Regression Model Training Summary without cross validation:\"+ '\\033[0m')\n",
    "        print(\" \")\n",
    "        print(\"Intercept: %s\" % str(fitModel.intercept))\n",
    "        print(\"\")\n",
    "        print(\"Coefficients: \")\n",
    "        coeff_array = fitModel.coefficients.toArray()\n",
    "        # Convert from numpy array to list\n",
    "        coeff_list = []\n",
    "        for x in coeff_array:\n",
    "            coeff_list.append(float(x))\n",
    "        result = spark.createDataFrame(zip(input_columns,coeff_list), schema=['feature','coeff'])\n",
    "        print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "\n",
    "\n",
    "\n",
    "        # Summarize the model over the training set and print out some metrics\n",
    "        print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "        print(\"objectiveHistory: (scaled loss + regularization) at each iteration \\n %s\" % str(trainingSummary.objectiveHistory))\n",
    "        print(\"\")\n",
    "        \n",
    "        # Print the Errors\n",
    "        print(\"Training RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "        print(\"Training r2: %f\" % trainingSummary.r2)\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "        # Now load the test results\n",
    "        test_results = fitModel.evaluate(test)\n",
    "\n",
    "        # And print them\n",
    "        print(\"Test RMSE: {}\".format(test_results.rootMeanSquaredError))\n",
    "        print(\"Test r2: {}\".format(test_results.r2))\n",
    "        print(\"\")\n",
    "        \n",
    "        #Now train with cross val\n",
    "        paramGrid = (ParamGridBuilder() \\\n",
    "#              .addGrid(regressor.maxIter, [10, 15]) \\\n",
    "             .addGrid(regressor.regParam, [0.1, 0.01]) \\\n",
    "             .build())\n",
    "        \n",
    "        #Evaluator\n",
    "        revaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "        \n",
    "        #Cross Validator requires all of the following parameters:\n",
    "        crossval = CrossValidator(estimator=regressor,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=revaluator,\n",
    "                                  numFolds=2) # 3 is best practice\n",
    "        \n",
    "        print('\\033[1m' + \"Linear Regression Model Summary WITH cross validation:\"+ '\\033[0m')\n",
    "        print(\" \")\n",
    "        # Run cross validations\n",
    "        fitModel = crossval.fit(train)\n",
    "        \n",
    "        #save model\n",
    "        global LR_BestModel \n",
    "        LR_BestModel = fitModel.bestModel\n",
    "        \n",
    "        print(\"Coefficients: \")\n",
    "        coeff_array = LR_BestModel.coefficients.toArray()\n",
    "        # Convert from numpy array to list\n",
    "        coeff_list = []\n",
    "        for x in coeff_array:\n",
    "            coeff_list.append(float(x))\n",
    "        result = spark.createDataFrame(zip(input_columns,coeff_list), schema=['feature','coeff'])\n",
    "        print(result.orderBy(result[\"coeff\"].desc()).show(truncate=False))\n",
    "        \n",
    "        # Get Model Summary Statistics\n",
    "        ModelSummary = fitModel.bestModel.summary\n",
    "        \n",
    "        print(\"Coefficient Standard Errors: \")\n",
    "        coeff_ste = ModelSummary.coefficientStandardErrors\n",
    "        result = spark.createDataFrame(zip(input_columns,coeff_ste), schema=['feature','coeff std error'])\n",
    "        print(result.orderBy(result[\"coeff std error\"].desc()).show(truncate=False))\n",
    "        print(\" \")\n",
    "        print(\"P Values: \") \n",
    "        # Then zip with input_columns list and create a df\n",
    "        pvalues = ModelSummary.pValues # this returns a list\n",
    "        result = spark.createDataFrame(zip(input_columns,pvalues), schema=['feature','P-Value'])\n",
    "        print(result.orderBy(result[\"P-Value\"].desc()).show(truncate=False))\n",
    "        print(\" \")\n",
    "        \n",
    "        # Use test set here so we can measure the accuracy of our model on new data\n",
    "        ModelPredictions = fitModel.transform(test)\n",
    "        \n",
    "        # cvModel uses the best model found from the Cross Validation\n",
    "        # Evaluate best model\n",
    "        test_results = revaluator.evaluate(ModelPredictions)\n",
    "        print('RMSE:', test_results)\n",
    "    \n",
    "        # Set the column names to match the external results dataframe that we will join with later:\n",
    "        columns = ['Regressor', 'Result']\n",
    "        \n",
    "        # Format results and return\n",
    "        rmse_str = [str(test_results)] #make this a string and convert to a list\n",
    "        Mtype = [Mtype] #make this a string\n",
    "        result = spark.createDataFrame(zip(Mtype,rmse_str), schema=columns)\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Add parameters of your choice here:\n",
    "        if Mtype in(\"RandomForestRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder() \\\n",
    "#                            .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "#                            .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "                           .addGrid(regressor.numTrees, [5, 20])\n",
    "                         .build())\n",
    "\n",
    "        # Add parameters of your choice here:\n",
    "        if Mtype in(\"GBTRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder() \\\n",
    "#                          .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20]) \\\n",
    "                         .addGrid(regressor.maxIter, [10, 15])\n",
    "                         .build())\n",
    "\n",
    "        # Add parameters of your choice here:\n",
    "        if Mtype in(\"DecisionTreeRegressor\"):\n",
    "            paramGrid = (ParamGridBuilder() \\\n",
    "#                          .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20, 40]) \\\n",
    "                         .build())\n",
    "\n",
    "        #Cross Validator requires all of the following parameters:\n",
    "        crossval = CrossValidator(estimator=regressor,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=RegressionEvaluator(metricName=\"rmse\"),\n",
    "                                  numFolds=2) # 3 is best practice\n",
    "        # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "        fitModel = crossval.fit(train)\n",
    "        \n",
    "        # Get Best Model\n",
    "        BestModel = fitModel.bestModel\n",
    "\n",
    "        # FEATURE IMPORTANCES\n",
    "        # Estimate of the importance of each feature.\n",
    "        # Each featureâ€™s importance is the average of its importance across all trees \n",
    "        # in the ensemble The importance vector is normalized to sum to 1. \n",
    "        print(\" \")\n",
    "        print('\\033[1m' + Mtype,\" Feature Importances\"+ '\\033[0m')\n",
    "        print(\"(Scores add up to 1)\")\n",
    "        print(\"Lowest score is the least important\")\n",
    "        print(\" \")\n",
    "        featureImportances = BestModel.featureImportances.toArray()\n",
    "        # Convert from numpy array to list\n",
    "        imp_scores = []\n",
    "        for x in featureImportances:\n",
    "            imp_scores.append(float(x))\n",
    "        # Then zip with input_columns list and create a df\n",
    "        result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "        print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "        \n",
    "        #Create Global Variables for feature importances and models\n",
    "        if Mtype in(\"DecisionTreeRegressor\"):\n",
    "            global DT_featureImportances\n",
    "            DT_featureImportances = BestModel.featureImportances.toArray()\n",
    "            global DT_BestModel \n",
    "            DT_BestModel = fitModel.bestModel\n",
    "        if Mtype in(\"GBTRegressor\"):\n",
    "            global GBT_featureImportances\n",
    "            GBT_featureImportances = BestModel.featureImportances.toArray()\n",
    "            global GBT_BestModel \n",
    "            GBT_BestModel = fitModel.bestModel\n",
    "        if Mtype in(\"RandomForestRegressor\"):\n",
    "            global RF_featureImportances\n",
    "            RF_featureImportances = BestModel.featureImportances.toArray()\n",
    "            global RF_BestModel \n",
    "            RF_BestModel = fitModel.bestModel\n",
    "                    \n",
    "        # Set the column names to match the external results dataframe that we will join with later:\n",
    "        columns = ['Regressor', 'Result']\n",
    "        \n",
    "        # Make predictions.\n",
    "        predictions = fitModel.transform(test)\n",
    "        # Select (prediction, true label) and compute test error\n",
    "        evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        rmse_str = [str(rmse)] #make this a string and convert to a list\n",
    "        Mtype = [Mtype] #make this a string\n",
    "        result = spark.createDataFrame(zip(Mtype,rmse_str), schema=columns)\n",
    "        # Clean up the Result column and output\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLinear Regression Model Training Summary without cross validation:\u001b[0m\n",
      " \n",
      "Intercept: -10116.004025693008\n",
      "\n",
      "Coefficients: \n",
      "+--------------+-------------------+\n",
      "|feature       |coeff              |\n",
      "+--------------+-------------------+\n",
      "|median_income |211304.87611811914 |\n",
      "|households    |112643.96027716424 |\n",
      "|total_bedrooms|-11925.268188423983|\n",
      "|population    |-100433.28430789644|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "numIterations: 0\n",
      "objectiveHistory: (scaled loss + regularization) at each iteration \n",
      " [0.0]\n",
      "\n",
      "Training RMSE: 80922.977563\n",
      "Training r2: 0.506518\n",
      "\n",
      "Test RMSE: 82658.54762537872\n",
      "Test r2: 0.49197189400279184\n",
      "\n",
      "\u001b[1mLinear Regression Model Summary WITH cross validation:\u001b[0m\n",
      " \n",
      "Coefficients: \n",
      "+--------------+-------------------+\n",
      "|feature       |coeff              |\n",
      "+--------------+-------------------+\n",
      "|median_income |211304.89373411733 |\n",
      "|households    |112640.01073348867 |\n",
      "|total_bedrooms|-11922.82969876115 |\n",
      "|population    |-100431.72420279558|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "Coefficient Standard Errors: \n",
      "+--------------+------------------+\n",
      "|feature       |coeff std error   |\n",
      "+--------------+------------------+\n",
      "|households    |5224.33661784282  |\n",
      "|total_bedrooms|4270.908118249787 |\n",
      "|population    |2591.4664576874584|\n",
      "|median_income |1908.1971605642163|\n",
      "+--------------+------------------+\n",
      "\n",
      "None\n",
      " \n",
      "P Values: \n",
      "+--------------+--------------------+\n",
      "|feature       |P-Value             |\n",
      "+--------------+--------------------+\n",
      "|total_bedrooms|0.005251208571089405|\n",
      "|population    |0.0                 |\n",
      "|households    |0.0                 |\n",
      "|median_income |0.0                 |\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "RMSE: 82658.54447137762\n",
      " \n",
      "\u001b[1mDecisionTreeRegressor  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+--------------------+\n",
      "|feature       |score               |\n",
      "+--------------+--------------------+\n",
      "|median_income |0.9367302110788684  |\n",
      "|population    |0.026199864685649336|\n",
      "|households    |0.023461098461117137|\n",
      "|total_bedrooms|0.013608825774365138|\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mRandomForestRegressor  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+--------------------+\n",
      "|feature       |score               |\n",
      "+--------------+--------------------+\n",
      "|median_income |0.922040732747812   |\n",
      "|population    |0.030790884354880615|\n",
      "|households    |0.030086532813234045|\n",
      "|total_bedrooms|0.017081850084073473|\n",
      "+--------------+--------------------+\n",
      "\n",
      "None\n",
      " \n",
      "\u001b[1mGBTRegressor  Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n",
      "+--------------+-------------------+\n",
      "|feature       |score              |\n",
      "+--------------+-------------------+\n",
      "|median_income |0.4492837945113075 |\n",
      "|population    |0.30003881407655986|\n",
      "|households    |0.1615187685268038 |\n",
      "|total_bedrooms|0.08915862288532887|\n",
      "+--------------+-------------------+\n",
      "\n",
      "None\n",
      "+---------------------+------+\n",
      "|Regressor            |Result|\n",
      "+---------------------+------+\n",
      "|LinearRegression     |82658 |\n",
      "|DecisionTreeRegressor|83066 |\n",
      "|RandomForestRegressor|81104 |\n",
      "|GBTRegressor         |77594 |\n",
      "+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run!\n",
    "regressors = [\n",
    "                LinearRegression()\n",
    "                ,DecisionTreeRegressor()\n",
    "                ,RandomForestRegressor()\n",
    "                ,GBTRegressor()\n",
    "                ] \n",
    "    \n",
    "#set up your results table\n",
    "columns = ['Regressor', 'Result']\n",
    "vals = [(\"Place Holder\",\"N/A\")]\n",
    "results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "for regressor in regressors:\n",
    "    new_result = RegressTrainEval(regressor)\n",
    "    results = results.union(new_result)\n",
    "results = results.where(\"Regressor!='Place Holder'\")\n",
    "results.show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take aways\n",
    "\n",
    "Looks like median income was the most influencial feature in the dataset which we can see by looking at the coefficients and feature important scores, but all features had significance accross the board in the linear model. \n",
    "\n",
    "The Gradient Boosted regressor had the lowest root mean squared error (rmse) which would make it the best performing model. \n",
    "\n",
    "Now let's see how our predictions were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------------+-------------------+-------------------+\n",
      "|            features|   label|        prediction|         difference|         diff perct|\n",
      "+--------------------+--------+------------------+-------------------+-------------------+\n",
      "|[4.86753445045558...|452600.0| 366144.2201570906| -86455.77984290943|-19.102028246334385|\n",
      "|[7.00940893270863...|358500.0| 446618.5931428357|  88118.59314283571| 24.579802829242876|\n",
      "|[5.25227342804663...|352100.0|452439.46967601264| 100339.46967601264| 28.497435295658235|\n",
      "|[5.46383180502561...|341300.0| 286106.7571219872|-55193.242878012825|-16.171474619986178|\n",
      "|[5.63835466933374...|342200.0| 210938.9541990841| -131261.0458009159| -38.35799117501926|\n",
      "|[5.36597601502185...|269700.0|232692.01193453668|-37007.988065463316|-13.721908811814354|\n",
      "|[6.19440539110467...|299200.0| 272241.3730509634| -26958.62694903661| -9.010236279758226|\n",
      "|[6.53378883793334...|241400.0| 262384.3177869363|   20984.3177869363|  8.692757989617357|\n",
      "|[6.50128967054038...|226700.0|149661.55801019882| -77038.44198980118|-33.982550502779524|\n",
      "|[6.56244409369372...|261100.0|  230395.391125993| -30704.60887400701|-11.759712322484493|\n",
      "+--------------------+--------+------------------+-------------------+-------------------+\n",
      "\n",
      "None\n",
      "+-------+------------------+\n",
      "|summary|        diff perct|\n",
      "+-------+------------------+\n",
      "|  count|                10|\n",
      "|   mean|-8.033590584365793|\n",
      "| stddev|22.377488846255183|\n",
      "|    min|-38.35799117501926|\n",
      "|    max|28.497435295658235|\n",
      "+-------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "test = final_data.limit(10)\n",
    "predictions = GBT_BestModel.transform(test)\n",
    "predictions = predictions.withColumn(\"difference\",predictions.prediction-predictions.label) \\\n",
    "                         .withColumn(\"diff perct\",((predictions.prediction-predictions.label)/predictions.label)*100)\n",
    "print(predictions.show())\n",
    "print(predictions.describe(['diff perct']).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output above how much difference there was between the actual house price (label) and the prediction one as both a raw dollar value and the percentage difference. The summary we printed out shows that our model, on average was off by about -9 % which means that the model under valued houses since it's a negative value. These are really great statistics to be able to deliver to a client who does have any knowledge about machine learning and wants to know how useful or \"good\" your model is. \n",
    "\n",
    "If you want to save your model you can simply..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this path will create the folder if it does not exist\n",
    "# I also like to create a unique identifier using a timestamp so i know when the model was created\n",
    "from datetime import datetime\n",
    "timestamp = str(datetime.now()) #return local time\n",
    "path = 'Models/LRModel_'+timestamp\n",
    "LR_BestModel.save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
